---
layout: default
title: "Jiahao Cao's homepage"
description: Machine Learning
theme: jekyll-theme-cayman
---


# Report on: Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks

# 0. Overall introduction

## 0.1 GANs progression

 Generative Adversarial Networks(GANs) was first introduced by Goodfellow et al, 2014[<sup>1</sup>](#refer-anchor-1). They simultaneously trained two models:

* a generative model $G$ that captures the data distribution
* a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$.

with a minimax optimization using backpropagation and dropout algorithm:

 $$\quad\quad\quad\min _{G} \max _{P} V(D, G)=\mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}_{\boldsymbol{z} \sim p_{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))]$$

GANs became an powerful tool to build generative models  without any Markov chains or approximate inference.

Later, there are many works on GANs and some are mostly mentioned:

■ DC GAN (Radford et al, 2016)

■ Improved Training of GANs (Salimans et al, 2016)

■ WGAN, WGAN-GP, Progressive GAN, SN-GAN, SAGAN

■ BigGAN, BigGAN-Deep, StyleGAN, StyleGAN-v2, VIB-GAN...

This progress can be demonstrated by a tweet from lan Goodfellow:

<img src="\Images\image-20200716230358276.png" alt="image-20200716230358276" style="zoom:30%;" />

These are faces generated by the algorithms including original GAN, DCGAN and StyleGAN. As we can see, for just about five years, GANs can generate extremely realistic and high resolution images.

## 0.2 Why this paper?

DCGAN is important because it works for pretty realistic pictures for the first time for a GAN model in the history.

There are two other reasons that I decide to write a report for this paper:

First, this paper describes its architectural in a really clear way.

There is always a lot of emphasis on architectural and engineering details in GANs.  This DCGANs paper shows how convolutional layers can be used with GANs and provides a series of additional architectural guidelines for doing this.

Second, many additional topics are involved in this paper. For example: visualizing GAN features, latent space interpolation and using discriminator features to train classifiers. These additional topics are really essential for researchers to go deeper in GANs. Also, it is the first time that some topics are investigated for purely unsupervised models.

# 1. The Gap between CNNs and unsupervised learning

## 1.1 Previous works and problems

Convolutional networks (CNNs) has seen huge adoption in computer vision applications. Generative adversarial networks(GANs)[<sup>1</sup>](#refer-anchor-1) by Goodfellow et al. showed the way to build unsupervised generative model based on well-developed neural networks instead of maximum likelihood techniques.

For unsupervised representation learning, methods such as K-means and auto-encoders have been shown to learn good feature representations from image pixels. For generative image tasks,  generating natural images of the real world have had not much success until recently.  However, available methods are suffering from some problems. Auto-encoding variational bayes suffers from getting blurry samples and Generative Adversarial Networks (Goodfellow et al., 2014) generated images suffers from being noisy and incomprehensible.  

Combing GANs with CNNs is not a new idea. The original GANs paper[<sup>1</sup>](#refer-anchor-1) used a convolutional discriminator to CIFAR-10 dataset;   LAPGAN[<sup>2</sup>](#refer-anchor-2) by Denton et al., 2015 used a cascade of convolutional networks  within a Laplacian pyramid framework to generate images.  But GANs have been known to be unstable to train, often resulting in generators that produce nonsensical outputs. Some works had success with generating natural images but they have not leveraged the generators for supervised tasks.

**Current problems/difficulties** can be summarized as:

*  Unsupervised learning with CNNs has received less attention.  
*  GANs have been known to be unstable to train, often resulting in generators that produce nonsensical outputs.
*  There has been very limited published research in trying to understand and visualize what GANs
   learn, and the intermediate representations of multi-layer GANs.  

### 1.2 Target of this work

Find a way to **build good image representations** using Generative Adversarial Networks (GANs). Later, extracted features can be used for supervised tasks.  

# 2. The contributions of this paper

According to this paper, the contributions are:

* (1) Propose and evaluate a set of constraints on the architectural topology of Convolutional
  GANs that make them stable to train in most settings and name this class of architectures
  **Deep Convolutional GANs (DCGAN)**.
* (2) **Use the trained discriminators for image classification tasks**, showing competitive performance with other unsupervised algorithms.  
* (3) **Visualize the filters learnt by GANs** and empirically show that specific filters have
  learned to draw specific objects.  
* (4) Show that the generators have interesting **vector arithmetic properties** allowing for easy
  manipulation of many semantic qualities of generated samples.  

In this section, I will mainly discuss the first contribution: DCGAN. The other contributions will be demonstrated in later sections with experiments.

### Deep Convolutional GANs (DCGAN)

With specific architecture,  DCGAN can result in stable training across a range of datasets and allow for training higher resolution and deeper generative models.  The core in this approach is to adopt and modify **three recently demonstrated changes to CNN architectures**, namely:

| 1.                   | **All convolutional net**                                    |
| -------------------- | ------------------------------------------------------------ |
| Related work         | Springenberg et al., 2014[<sup>3</sup>](#refer-anchor-3)     |
| Description          | Replaces deterministic spatial pooling functions (such as max pooling) with strided convolutions, allowing the network to learn its own spatial downsampling. |
| Application in DCGAN | Use strided convolutions in discriminator and fractional-strided convolutions in generator. |

| **2**.               | **Eliminating fully connected layers on top of convolutional features** |
| -------------------- | ------------------------------------------------------------ |
| Related work         | Global average pooling by Mordvintsev et al., 2015[<sup>4</sup>](#refer-anchor-4) |
| Description          | Eliminating fully connected layers on top of convolutional features.  A strong example is the global average pooling. It has been found that global average pooling increased model stability but hurt convergence speed. |
| Application in DCGAN | For the first layer of the generator, which takes a uniform noise distribution Z as input, could be called fully connected as it is just a matrix multiplication. The result is reshaped into a 4-dimensional tensor and used as the start of the convolution stack. For the discriminator, the last convolution layer is flattened and then fed into a single sigmoid output. |

| **3**.               | **Batch Normalization**                                      |
| -------------------- | ------------------------------------------------------------ |
| Related work         | Ioffe & Szegedy, 2015[<sup>5</sup>](#refer-anchor-5)         |
| Description          | Stabilizes learning by normalizing the input to each unit to have zero mean and unit variance. This helps deal with training problems that arise due to poor initialization and helps gradient flow in deeper models. This proved critical to get deep generators to begin learning, preventing the generator from collapsing all samples to a single point which is a common failure mode observed in GANs. |
| Application in DCGAN | Applying batchnorm to all layers but generator output layer and the discriminator input layer. |

Other details in this architecture are:

* **ReLU activation**: Used in the generator with the exception of the output layer.
* **Tanh activation**: Used in the output layer of the generator.
* **leaky rectified activation**: Used Within the discriminator.

Some examples of DCGAN are given in the following experiments.

# 3. Experiments to generate images

## 3.1 Implementation

Alec Radford, who is the first author of this paper, has a [GitHub repository](https://github.com/Newmu/dcgan_code) about some code and implementations.

The code is mainly written in **Python** and based the following packages/libraries:

* numpy: The fundamental package for scientific computing with Python
* theano: A Python library that are used extensively by researchers in the deep learning domain (Not so popular now). It allows users to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently.  About the convolution operators in theano, see [this document](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html).
* [lib](https://github.com/Newmu/dcgan_code/tree/master/lib): A package written by Alec Radford that contains some basic functions used in this paper. For example, activation functions and data utilization functions.
* sklearn: A famous machine learning  package that Built on NumPy, SciPy, and matplotlib.

Also, implementations based on Torch, Chainer and Tensorflow are provided.

Three datasets were used to train three different DCGANs in this section, namely:

* Large-scale Scene Understanding (LSUN)  
* ImageNet-1k   
* A newly assembled Faces dataset  

## 3.2 General settings

* **Pre-processing**: Just scale the training images to the range of the $tanh$ activation function $[-1, 1]$ .
* **Weights initialization**: a zero-centered Normal distribution with standard deviation 0.02 for all weights.

* **Optimization**:
  *  mini-batch stochastic gradient descent (SGD)  with mini-batch size 128
  *  Adam optimizer with tuned hyperparameters:
     *  learning rate: 0.002 (the suggested rate 0.001 by Adam is too high)
     *  momentum term: 0.5 (the suggested value 0.9 by Adam can result in training oscillation and instability)
* **Activation parameter**:  LeakyReLU slope$= 0.2$.

## 3.3 Model for LSUN

#### 3.3.1 Dataset

* **Source**: LSUN[<sup>7</sup>](#refer-anchor-7) is a dataset by Yu, Fisher, et al. 2015 that contains around one million labeled images for each of 10 scene categories and 20 object categories.
* **Raw data**: the LSUN bedrooms dataset containing a little over 3  million 64 x 64 training examples.  

#### 3.3.2 Model architecture

Generator with 100 dimensional noise $z$ is shown in figure 1 in the paper:

<img src="\Images\image-20200715223626780.png" alt="image-20200715223626780" style="zoom:40%;" />

However, the paper doesn't describe the architecture of the discriminator and the GitHub by Alec Radford also has no code for LSUN dataset. A model can be found in this [Torch implementation](https://github.com/soumith/dcgan.torch/blob/master/main.lua), which is based on **Lua**, by another author of this paper: Soumith Chintala. See this [document](https://github.com/torch/nn/blob/master/doc/convolution.md) for convolutional operations in torch.

Notice some default hyper parameters are a little different from above discussions. For example, batch size chosen by  Soumith Chintala is 64, not 128.

In the following description, kernel size is denoted by (depth/input_channels, height, width).

* **Some default training parameters:**

  ```lua
  batchSize = 64,			-- training batch size
  nz = 100,               -- number of dimension for Z
  ngf = 64,               -- number of generator filters in first convolutional layer
  ndf = 64,               -- number of discrim filters in first convolutional layer
  niter = 25,             -- number of iterations at starting learning rate
  lr = 0.0002,            -- initial learning rate for adam
  beta1 = 0.5,            -- momentum term of adam
  noise = 'normal',       -- uniform / normal noise. normal(0, 1) is used in this code.
  ```

* **Generator** with 100 dimensional noise input:

  * 512 (100,4,4) fractionally-strided convolution kernel (Equivalently, project using  a 100 x (4 x 4 x 512) matrix and reshape to 512 x 4 x 4), Batch Normalization, ReLU activation. Output:  512 x 4 x 4
  * 256 (512,4,4) fractionally-strided convolution kernel, Stride= 2, padding =1, Batch Normalization, ReLU activation. Output:  256 x 8 x 8
  * 128 (256,4,4) fractionally-strided convolution kernel, Stride= 2, padding =1, Batch Normalization, ReLU activation. Output: 128 x 16 x 16
  * 64 (128,4,4) fractionally-strided convolution kernel, Stride= 2, padding =1, Batch Normalization, ReLU activation. Output:  64 x 32 x 32
  * 3 (64,4,4) fractionally-strided convolution kernel, Stride= 2, padding =1, tanh activation. Output: 3 x 64 x 64

* **Discriminator**  with 3(channels) x 64 x 64 image input:

  * 64 (3,4,4) convolution kernel, Stride= 2, padding =1. LeakyReLU activation. Output: 64 x 32 x 32
  * 128 (64,4,4) convolution kernel, Stride= 2, padding =1. Batch Normalization.  LeakyReLU activation. Output: 128 x 16 x 16
  * 256 (128,4,4) convolution kernel, Stride= 2, padding =1. Batch Normalization.  LeakyReLU activation. Output: 256 x 8 x 8
  * 512 (256,4,4) convolution kernel, Stride= 2, padding =1. Batch Normalization.  LeakyReLU activation. Output: 512 x 4 x 4
  * (512,4,4) convolution kernel(Equivalently, flatten it and project using  a 1 x (4 x 4 x 512) matrix), Sigmoid activation. Output 1 x 1 x 1

#### 3.3.3 Purpose of training on LSUN:

* To demonstrate how this model scales with more data (over 3M in this case) and higher resolution generation.
* To demonstrate that this model is not producing high quality samples via simply overfitting/memorizing training examples.   

#### 3.3.4 Results and some conclusions

**Samples from one epoch of training** and **Samples after convergence (five epochs of training)**

<img src="\Images\image-20200717214427258.png" alt="image-20200717214427258" style="zoom: 22%;" />        <img src="\Images\image-20200717214450837.png" alt="image-20200717214450837" style="zoom:22%;" />

**Some conclusions:**

* This model, after one training pass through the dataset with SGD and a small learning rate, is not likely to "memorize" training examples.
* After five epochs of training, there appears to be evidence of visual under-fitting via repeated noise textures across multiple samples such as the base boards of some of the beds.  

#### 3.3.5 De-duplication

To further decrease the likelihood of the generator memorizing input examples, this paper performed a de-duplication process using 3072-128-3072 de-noising dropout regularized RELU autoencoder on 32x32 down sampled center-crops of training examples.

As a result, approximately 275,000  near duplicates  are removed, suggesting a high recall.  

## 3.4 Model for FACES

#### 3.4.1 Dataset

* **Source**: This dataset is obtained by scraping images containing human faces from random web image queries of peoples names. The people names were acquired from [dbpedia](https://wiki.dbpedia.org/).  
* **Description**:
  * Human face images with names
  * Born in modern era
* **Raw data**: 3M 64 x 64 images from 10K people
* **Pre-processing**: Use OpenCV face detector to get approximately 350,000 face boxes with sufficiently high resolution.

#### 3.4.2 Model architecture

The implementation for this model by Alec Radford is in this [link](https://github.com/Newmu/dcgan_code/blob/master/faces/train_uncond_dcgan.py). Notice this code is mainly using **theano** in python. Also, Adam is used as the optimizer in this implementation.

* **Some default training parameters:**

  ```python
  k = 1             # number of discrim updates for each gen update
  l2 = 1e-5         # l2 weight decay
  b1 = 0.5          # momentum term of adam
  nc = 3            # number of channels in image
  nbatch = 128      # number of examples in batch
  npx = 64          # number of pixels width/height of images
  nz = 100          # number of dim for Z
  ngf = 128         # number of generator filters in first convolutional layer
  ndf = 128         # number of discriminator filters in first convolutional layer
  niter = 25        # number of iterations at starting learning rate
  niter_decay = 0   # number of iterations to linearly decay learning rate to zero
  lr = 0.0002       # initial learning rate for adam
  ntrain = 350000   # number of examples to train on
  ```

* **Generator** with 100 dimensional noise input:

  * Project with 100 x (4 x 4 x 1024)  matrix, ReLU activation, reshape to 1024 x 4 x 4
  * 512 (1024,5,5) fractionally-strided convolution kernel, Stride= 2, padding =2, Batch Normalization, ReLU activation. Output:  512 x 8 x 8
  * 256 (512,5,5) fractionally-strided convolution kernel, Stride= 2, padding =2, Batch Normalization, ReLU activation. Output: 256 x 16 x 16
  * 128 (256,5,5) fractionally-strided convolution kernel, Stride= 2, padding =2, Batch Normalization, ReLU activation. Output:  128 x 32 x 32
  * 3 (128,5,5) fractionally-strided convolution kernel, Stride= 2, padding =2, tanh activation. Output: 3 x 64 x 64

* **Discriminator**  with 3(channels) x 64 x 64 image input:

  * 128 (3,5,5) convolution kernel, Stride= 2, padding =2. LeakyReLU activation. Output: 128 x 32 x 32
  * 256 (128,5,5) convolution kernel, Stride= 2, padding =2. Batch Normalization.  LeakyReLU activation. Output: 256 x 16 x 16
  * 512 (256,5,5) convolution kernel, Stride= 2, padding =2. Batch Normalization.  LeakyReLU activation. Output: 512 x 8 x 8
  * 1024 (512,5,5) convolution kernel, Stride= 2, padding =2. Batch Normalization.  LeakyReLU activation. Output: 1024 x 4 x 4
  * Flatten to 8192, dot product with 8192 dimensional vector, Sigmoid activation. Output 1

## 3.5 Model for  ImageNet-1k

#### 3.5.1 Dataset

* **source**: ImageNet-1k[<sup>8</sup>](#refer-anchor-8). See more details on [Summary and Statistics of Imagenet](http://image-net.org/about-stats).
* **Description**: The ImageNet dataset consists of more than 14M images, divided into approximately 22k different labels/classes.
* **Raw data**: 32 × 32 min-resized center crops images.

#### 3.5.2 Model architecture

Alec Radford  provided his **pretrained**  parameters in [here](https://github.com/Newmu/dcgan_code/tree/master/models/imagenet_gan_pretrain_128f_relu_lrelu_7l_3x3_256z) and model code in [here](https://github.com/Newmu/dcgan_code/blob/master/imagenet/load_pretrained.py).

* **Some default training parameters:**

  ```python
  nz = 256		# number of dim for Z
  nc = 3		    # number of channels in image
  npx = 32		# number of pixels width/height of images
  ngf = 128         # number of generator filters in first convolutional layer
  ndf = 128         # number of discriminator filters in first convolutional layer
  ```

* **Generator** with 256 dimensional noise input:

  * Project with 256 x 8192  matrix, Batch Normalization, ReLU activation, reshape to 512 x 4 x 4

  * 512 (512,3,3) fractionally-strided convolution kernel, Stride= 2, padding =1, Batch Normalization, ReLU activation. Output:  512 x 8 x 8
  * 256 (512,3,3) fractionally-strided convolution kernel, Stride= 1, padding =1, Batch Normalization, ReLU activation. Output: 256 x 8 x 8
  * 256 (256,3,3) fractionally-strided convolution kernel, Stride= 2, padding =1, Batch Normalization, ReLU activation. Output:  256 x 16 x 16
  * 128 (256,3,3) fractionally-strided convolution kernel, Stride= 1, padding =1, Batch Normalization, ReLU activation. Output:  128 x 16 x 16
  * 128 (128,3,3) fractionally-strided convolution kernel, Stride= 2, padding =1, Batch Normalization, ReLU activation. Output:  128 x 32 x 32
  * 3 (128,3,3) fractionally-strided convolution kernel, Stride= 1, padding =1, tanh activation. Output: 3 x 32 x 32

* **Discriminator**  with 3(channels) x 32 x 32 image input:

  * 128 (3,3,3) convolution kernel, Stride= 1, padding =1. LeakyReLU activation. Output: 128 x 32 x 32

  * 128 (128,3,3) convolution kernel, Stride= 2, padding =1. Batch Normalization.  LeakyReLU activation. Output: 128 x 16 x 16

  * 256 (128,3,3) convolution kernel, Stride= 1, padding =1. Batch Normalization.  LeakyReLU activation. Output: 256 x 16 x 16

  * 256 (256,3,3) convolution kernel, Stride= 2, padding =1. Batch Normalization.  LeakyReLU activation. Output: 256 x 8 x 8

  * 512 (256,3,3) convolution kernel, Stride= 1, padding =1. Batch Normalization.  LeakyReLU activation. Output: 512 x 8 x 8

  * 512 (512,3,3) convolution kernel, Stride= 2, padding =1. Batch Normalization.  LeakyReLU activation. Output: 512 x 4 x 4

  * Flatten to 8192, dot product with 8192 dimensional vector. sigmoid activation. Output: 1

#### 3.5.3 Results

See figure 11 in the paper.  Also, compared with a previous model: pixel RNN, which also works on this 32 x 32 ImageNet dataset, DCGAN produced more realistic looking images than the images generated by pixel RNN.

# 4. Apply DCGANs as a feature extractor on supervised datasets

## 4.1 Feature extractor

One common technique for evaluating the quality of unsupervised representation learning algorithms is to apply them as a feature extractor on supervised datasets and evaluate the performance of linear models fitted on top of these features.  

Here, this paper did several experiments using DCGANs as a feature extractor to show its performance.

## 4.2 Experiment on the CIFAR-10 dataset

#### 4.2.1 CIFAR-10 dataset

[The CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.

The dataset is divided into five training batches and one test batch, each with 10000 images.  The test batch contains exactly 1000 randomly-selected images from each class.

#### 4.2.2 Model

* The DCGAN **trained on ImageNet-1k** was used.
* **Extracted features**: the discriminator’s convolutional features from all layers, max pooling each layers representation to produce a 4 × 4 spatial grid.  These features are then flattened and concatenated to form a 28672 dimensional vector. (According to 3.5.2, there are $128*2+256*2+512*2=1792$ total depth, so the overall features are $1792*16 =28672  $)
* **Classifier**:  A regularized linear L2-SVM classifier is trained on top of these 28672 dimensional vectors.

This paper listed fitting results from several other models to make comparation:

$$\begin{array}{|c|c|c|c|}
\hline \text { Model } & \text { Accuracy } & \text { Accuracy (400 per class) } & \text { max # of features units } \\
\hline \text { 1 Layer K-means } & 80.6 \% & 63.7 \%(\pm 0.7 \%) & 4800 \\
\text { 3 Layer K-means Learned RF } & 82.0 \% & 70.7 \%(\pm 0.7 \%) & 3200 \\
\text { View Invariant K-means } & 81.9 \% & 72.6 \%(\pm 0.7 \%) & 6400 \\
\text { Exemplar CNN } & 84.3 \% & 77.4 \%(\pm 0.2 \%) & 1024 \\
\hline \text { DCGAN (ours) + L2-SVM } & 82.8 \% & 73.8 \%(\pm 0.4 \%) & 512 \\
\hline
\end{array}$$

#### 4.2.3 Conclusions

* This model out performing all K-means based approaches listed above.
* The performance of DCGANs is still less than that of Exemplar CNNs (Dosovitskiy et al., 2015[<sup>9</sup>](#refer-anchor-9))  
* Further improvements could be made by finetuning the discriminator’s representations  
* Since this DCGAN was never trained on CIFAR-10, this experiment also demonstrates the **domain robustness of the learned features**.

## 4.3 Experiment on classifying SVHN digits

#### 4.3.1 Dataset

Street View House Numbers dataset (SVHN)(Netzer et al., 2011[<sup>10</sup>](#refer-anchor-10))  contains over 600,000 labeled digits cropped from Street View images.

**A validation set** of 10,000 examples is split off from the non-extra set and use it for all hyperparameter and model selection. 1000 uniformly class distributed training examples are randomly selected as **training set**.

#### 4.3.2 Model

Alec Radford  provided his pretrained parameters in [here](https://github.com/Newmu/dcgan_code/tree/master/models/svhn_unsup_all_conv_dcgan_100z_gaussian_lr_0.0005_64mb) and model code in [here](https://github.com/Newmu/dcgan_code/blob/master/svhn/svhn_semisup_analysis.py).

A regularized linear L2-SVM classifier is trained on top of the same feature extraction pipeline used for CIFAR-10.  

#### 4.3.3 Results and conclusions

$$\text{Table : SVHN classification with 1000 labels}\\ \begin{array}{|c|c|}
\hline \text{Model} & \text{error rate} \\
\hline \text{KNN} & 77.93 \%$ \\
\text{TSVM} & 66.55 \% \\
\text{M1+KNN} & 65.63 \% \\
\text{M1+TSVM} & 54.33 \% \\
\text{M1+M2} & 36.02 \% \\
\hline \text{SWWAE without dropout} & 27.83 \% \\
\text{SWWAE with dropout} & 23.56 \% \\
\hline \text{DCGAN (ours) + L2-SVM} & 22.48 \% \\
\text{Supervised CNN with the same architecture} & 28.87 \% \text{(validation)} \\
\hline
\end{array}$$

* This algorithm achieves state of the art (for classification using 1000 labels) at 22.48% test error.
* The CNN architecture used in DCGAN is **not** the key contributing factor of the
  model’s performance:
  * Training a purely supervised CNN with the same architecture on the same data and optimizing using the method proposed by Bergstra & Bengio, 2012[<sup>11</sup>](#refer-anchor-11). It achieves a significantly higher 28.87% validation error.

## 5. Investigating and visualizing the internals of the networks

we always want to assess our model after training. However, **Evaluation for GANs is still an open problem**. Some methods like Nearest neighbors in pixel/feature space or log-likelihood metrics have already been shown to be insufficient.

In this section, several experiments are used to investigate the trained generators and discriminators. We will see that interesting applications can be developed using Z representations learned by DCGAN.

Also, it is the first time that such results was demonstrated in purely unsupervised models.  

## 5.1 Walking in the latent space

The idea of **latent space** has been used for a long time in both statistics and machine learning. A famous example is the variational auto encoder(VAE).

Now imagine there exist a latent space so that every real image corresponds to a point in this space. By walking in this latent space, equivalently we are walking on the manifold of real images. Thus if out model successfully learned a decent feature representation, we are hoping to find that walking in this latent space results in semantic changes to the image generations (such as objects being added and removed).

#### 5.1.1 Experiment

Use the DCGAN trained on  the LSUN bedrooms dataset.  Choose a series of  9 random points in $Z$ (random noise input) and do linear interpolation. Use these points to generate bedroom images with our generator.

#### 5.1.2 results and conclusions

See Figure 4 in the paper. Each image is generated by the DCGAN generator with a point in $Z$.

* We can see that the space learned has smooth transitions, with every image in the space plausibly looking like a bedroom.  

  ![image-20200716181802532](\Images\image-20200716181802532.png)

* In the 6th row in figure 4, a room without a window slowly transforming into a room with a giant window:

  ![image-20200718004144531](\Images\image-20200718004144531.png)

* In the 10th row in Figure 4, a TV slowly being transformed into a window:

  ![image-20200716184322559](\Images\image-20200716184322559.png)

Further researches found that instead of linear interpolation, interpolation on a spherical is more likely to produce better generated samples.

## 5.2 Visualizing the discriminator features

Similar to CNNs, an unsupervised DCGAN trained on a large image dataset can also learn a hierarchy of features that are interesting.

Using guided backpropagation as proposed by Springenberg et al., 2014[<sup>4</sup>](#refer-anchor-4), this paper shows that the **features learnt by the discriminator** activate on typical parts of a bedroom, like beds and windows. See the following figure:

* On the right, the first 6 learned convolutional features from the last convolution layer in the discriminator is Visualized.

* On the left, it is a random filter baseline.   

  <img src="\Images\image-20200718005228845.png" alt="image-20200718005228845" style="zoom:30%;" />

## 5.3 Manipulating The Generator Representation

In addition to the representations learnt by a discriminator, there is the question of **what representations the generator learns.**

#### 5.3.1 Forgetting To Draw Certain Objects

In order to explore the form that these representations take, this paper conducted an experiment to **attempt to remove windows from the generator completely**.

* **Experiment**:

  * On 150 samples, 52 window bounding boxes were drawn manually.  
  * On the second highest convolution layer features, logistic regression was fit to predict whether a feature activation was on a window (or not), by using the criterion that activations inside the drawn bounding boxes are positives and random samples from the same images are negatives.  
  * All feature maps with weights greater than zero (200 in total) were dropped from all spatial locations.  
  * Random new samples were generated with and without the feature map removal.  

* **Results and conclusions**: See figure 6 in the paper:

  ![image-20200718145519552](\Images\image-20200718145519552.png)

  * The first row: Samples without dropping feature maps.
  * The second row:  Samples with window dropout.
  * With the window dropout, the network mostly forgets to draw windows in the bedrooms, replacing them with other objects.  

#### 5.3.2 Vector arithmetic of face images

Work by Mikolov et al., 2013[<sup>12</sup>](#refer-anchor-12)  shows that for words and phrases, simple arithmetic operations revealed rich **linear structure** in representation space.  For example, vector(”King”) - vector(”Man”) + vector(”Woman”)  resulted in a vector whose nearest neighbor was the vector for Queen.  

This paper investigated whether similar structure emerges in the Z representation of generators for **visual concepts**.  

* **Experiment**:

  * For each of our three visual concepts, for example :"man with glasses", "man without glasses" and "woman without glasses", create three $z$ vectors.
  * Average three $z$ vectors for each concept and get $z_1,z_2,z_3$.
  * Compute $Y = z_1-z_2+z_3$ and feed $Y$ as input to the generator to get an image.
  * Sample $8$  noise samples from $\text{Uniform}(-0.25,0.25)$. Add these noise samples to $Y$ and use generator to generate corresponding $8$ images.

* **Results and conclusions**: See figure 7 and figure 8 in the paper.

  <img src="\Images\image-20200716205754026.png" alt="image-20200716205754026" style="zoom:30%;" />

  * Averaging the Z vector for three exemplars showed consistent and stable generations that semantically obeyed the arithmetic.  There exists some **linear structure** in Z space. Notice that Experiments working on only single samples per concept were unstable.

  * Another experiment demonstrated that face pose is also modeled linearly in Z space.



# 6. Conclusions and some personal thoughts

**Conclusions**:

In conclusion, GANs could be made to work well with specific architecture details. The DCGAN is capable of generating decent natural images and learning good feature representations.

There are still problems remain to solve, such as model instability in DCGAN and brittle architecture/hyperparameters.

In addition, there are many interesting works based on DCGAN like [Generate Pokemon with DCGAN](https://github.com/kvpratama/gan/tree/master/pokemon) and  [image completion](https://github.com/bamos/dcgan-completion.tensorflow).

**some personal thoughts:**

* Architecture is very important in deep learning algorithms. It is amazing to see that just putting some techniques and details together, we can get a powerful algorithm.
* Many machine learning and deep learning ideas are influenced by statistics. Before GANs, likelihood-based methods such as Pixel RNN and VAE are pretty popular for generative models. Generative models also naturally have much connection with Bayesian statistics. But in Bayesian computation, Monte Carlo is widely used, which usually takes a lot of time. GANs provide an attractive alternative to those likelihood-based techniques.
* The work about feature representation in this paper is really fascinating. I suppose that how to capture and utilize those hidden features might be an important task to build artificial intelligence.















# References

<div id="refer-anchor-1"></div>

[1].Goodfellow, Ian, et al. "Generative adversarial nets." *Advances in neural information processing systems*. 2014.

<div id="refer-anchor-2"></div>

[2].Denton, Emily L., Soumith Chintala, and Rob Fergus. "Deep generative image models using a￼ laplacian pyramid of adversarial networks." *Advances in neural information processing systems*. 2015.

<div id="refer-anchor-3"></div>

[3].Springenberg, Jost Tobias, et al. "Striving for simplicity: The all convolutional net." *arXiv preprint arXiv:1412.6806* (2014).

<div id="refer-anchor-4"></div>

[4].Mordvintsev, Alexander, Christopher Olah, and Mike Tyka. "Inceptionism: Going deeper into neural networks." (2015).

<div id="refer-anchor-5"></div>

[5].Ioffe, Sergey, and Christian Szegedy. "Batch normalization: Accelerating deep network training by reducing internal covariate shift." *arXiv preprint arXiv:1502.03167* (2015).

<div id="refer-anchor-6"></div>

[6].Nair, Vinod, and Geoffrey E. Hinton. "Rectified linear units improve restricted boltzmann machines." *ICML*. 2010.

<div id="refer-anchor-7"></div>

[7].Yu, Fisher, et al. "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop." *arXiv preprint arXiv:1506.03365* (2015).

<div id="refer-anchor-8"></div>

[8].Deng, Jia, et al. "Imagenet: A large-scale hierarchical image database." *2009 IEEE conference on computer vision and pattern recognition*. Ieee, 2009.

<div id="refer-anchor-9"></div>

[9].Dosovitskiy, Alexey, et al. "Discriminative unsupervised feature learning with exemplar convolutional neural networks." *IEEE transactions on pattern analysis and machine intelligence* 38.9 (2015): 1734-1747.

<div id="refer-anchor-10"></div>

[10].Netzer, Yuval, et al. "Reading digits in natural images with unsupervised feature learning." (2011).

<div id="refer-anchor-11"></div>

[11].Bergstra, James, and Yoshua Bengio. "Random search for hyper-parameter optimization." *The Journal of Machine Learning Research* 13.1 (2012): 281-305.

<div id="refer-anchor-12"></div>

[12].Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." *Advances in neural information processing systems*. 2013.
