---
layout: default
title: "Jiahao Cao's homepage"
description: Bayesian Statistics
theme: jekyll-theme-cayman
---
$
\newcommand{\E}{\mathbb{E}}
\newcommand{\P}{\mathbb{P}}
\newcommand{\N}{\mathcal{N}}
$
In my Bayesian class I was asked to do inference based Gaussian Mixture Model(GMM):

$$\begin{align*}
&Y_n | Z_n =k, \mu_k, \sigma_k^2 \sim \N(\mu_k, \sigma_k^2),\quad n\in\{1,2,\cdots,N\} \\
&Z_n\in\{1,\cdots, K\} \text{ and } P(Z_n=k) =\pi_k.
\end{align*}$$

Where $Y_1,\cdots,Y_N$ are observations and $Z,\mu\,\sigma^2,\pi$ are hidden. Our goal is make inference about $\mu,\sigma$ and $Z$.

We will use three different methods, namely

* EM algorithm
* Gibbs sampling with conjugate priors
* Variational inference with mean-field variational distribution

## 1. EM algorithm

The log-likelihood is:

$$
\log p(y, z | \mu, \sigma, \pi) \equiv \sum_{i=1}^{n} \sum_{j=1}^{K} 1_{\left[z_{i}=j\right]}\left\{\log \pi_{j}-\frac{1}{2} \log \sigma_{j}^{2}-\frac{1}{2} \frac{\left(y_{i}-\mu_{j}\right)^{2}}{\sigma_j^{2}}\right\}
$$

Write $\theta = (\mu\,\sigma^2,\pi)$.

### 1.1 E step

$$Q(\theta,\theta'):=\mathbb{E}\bigg\{\log p(y, z | \theta)\bigg| Y=y,\theta'\bigg\}=\sum_{i=1}^{n} \sum_{j=1}^{K} \mathbb{E}\bigg\{1_{\left[z_{i}=j\right]}\bigg| Y=y,\theta'\bigg\}\left\{\log \pi_{j}-\frac{1}{2} \log \sigma_{j}^{2}-\frac{1}{2} \frac{\left(y_{i}-\mu_{j}\right)^{2}}{\sigma_j^{2}}\right\}$$

So we need to find the conditional probability of $\{z_i=j\}$ given $\theta'$ and observations $y$. Notice the joint distribution of $(y, z | \mu, \sigma, \pi)$ shows that $(y_i,z_i) $
 are independent. So

 $$w_{ij}:=\P(z_i=j|y,\theta') =\P(z_i=j|y_i,\theta') =\frac{\pi'_{j} \N\left(y_{i} ; \mu'_{j}, \sigma_{j}^{'2}\right)}{\sum_{q=1}^{K} \pi'_{q} \N\left(y_{i}, \mu'_{q}, \sigma_{q}^{'2}\right)}$$

### 1.2 M step

$$\begin{array}{l}
\frac{\partial Q\left(\theta, \theta^{\prime}\right)}{\partial \pi_{j}}=\frac{1}{\pi_{j}}\left(\sum_{i=1}^{n} w_{i j}\right) \\
\frac{\partial Q\left(\theta, \theta^{\prime}\right)}{\partial \sigma_{j}^{2}}=\sum_{i=1}^{n} w_{i j}\left[-\frac{1}{2 \sigma_{j}^{2}}+\frac{\left(y_{i}-u_{j}\right)^{2}}{2\left(\sigma_j^{2}\right)^{2}}\right]=0 \\
\frac{\partial Q\left(\theta, \theta^{\prime}\right)}{\partial u_{j}}=\sum_{i=1}^{n} w_{i j} \cdot \frac{y_{i}-u_{j}}{\sigma_{j}^{2}}=0 \Rightarrow \mu_{j}=\frac{\sum_{i=1}^{n} w_{i j} y_{i}}{\sum_{i=1}^{n} w_{i j}}
\end{array}$$

$$\begin{array}{l}
\Rightarrow \quad \sum_{i=1}^{n} w_{i j}=\frac{1}{\sigma_{j}^{2}}\left[\sum_{i=1}^{n} y_{i}^{2} w_{i j}+\left(\sum_{i=1}^{n} w_{i j}\right) \mu_{j}^{2}-2\left(\sum_{i=1}^{n} w_{i j} y_{i}\right) \mu_{j}\right] \\
\quad\quad\Rightarrow \sigma_{j}^{2}=\left(\sum_{i=1}^{n} w_{i j}\right)^{-1}\left[\sum_{i=1}^{n} y_{i}^{2} w_{i j}-\left(\sum_{i=1}^{n} w_{i j}\right) \mu_{j}^{2}\right] \\
\Rightarrow \quad \frac{\partial Q(\theta, \theta^{\prime})}{\partial \pi_{j}}=\frac{1}{\pi_{j}}\left(\sum_{i=1}^{n} w_{i j}\right)-\frac{\sum_{i=1}^{n} w_{i k}}{1-\pi_{1}-\cdots-\pi_{k-1}}=0 \Rightarrow \pi_{j}=\frac{\sum_{i=1}^{n} w_{i k}}{\left(1-\pi_{1}-\cdots-\pi_{k-1}\right)\left(\sum_{i=1}^{n} w_{i j}\right)}\\
\quad\quad\Rightarrow \pi_j = \frac{1/\sum_{i=1}^{n} w_{i j}}{\sum_{q=1}^K(1/\sum_{i=1}^{n} w_{i q})}
\end{array}$$

In summary:

$$w_{ij}=\frac{\pi'_{j} \N\left(y_{i} ; \mu'_{j}, \sigma_{j}^{'2}\right)}{\sum_{q=1}^{K} \pi'_{q} \N\left(y_{i}, \mu'_{q}, \sigma_{q}^{'2}\right)}$$

$$
\begin{cases}
\mu_{j}=\frac{\sum_{i=1}^{n} w_{i j} y_{i}}{\sum_{i=1}^{n} w_{i j}}\\
\sigma_{j}^{2}=\left(\sum_{i=1}^{n} w_{i j}\right)^{-1}\left[\sum_{i=1}^{n} y_{i}^{2} w_{i j}-\left(\sum_{i=1}^{n} w_{i j}\right) \mu_{j}^{2}\right]\\
\pi_j = \frac{1/\sum_{i=1}^{n} w_{i j}}{\sum_{q=1}^K(1/\sum_{i=1}^{n} w_{i q})}
\end{cases}$$




## 2. Gibbs sampling

Now given the model and the corresponding priors:

$$\begin{align*}
&Y_n | Z_n =k, \mu_k, \sigma_k^2 \sim \N(\mu_k, \sigma_k^2) \\
&Z_n\in\{1,\cdots, K\} \text{ and } P(Z_n=k) =\pi_k \\
& \mu_k \sim \N(\mu_0, \sigma^2_0) \\
& \sigma_k^{-2} \sim \text{Gamma}(a_0, b_0) \\
& (\pi_1, \cdots, \pi_K)\sim \text{Dirichlet}(\alpha_1,\cdots,\alpha_K).
\end{align*}$$

We need to derive all the full conditional distributions. This is quite easy because there are some conjugacy.

* (1)

$$\P(y|-) = \prod_{i=1}^N \N  (y_i;\mu_{z_i},\sigma^2_{z_i}) $$

* (2)

$$
p(z |-)=\frac{\prod_{i=1}^N \left[ \prod_{j=1}^{K}\left[\pi_{j} \N\left(y_{i} ; \mu_{j} , \sigma^{\prime}\right)\right]^{I_{\left(z_{i}=j\right) }}\right]}{\prod_{i=1}^{N}\left[\sum_{j=1}^{K} \pi_{j} \N\left(y_{i} ; \mu_{j}, \sigma_{j}^{2}\right)\right]}
$$

Write

$$\begin{aligned}
Id(j):&=\left\{i: z_{i}=j\right\} \\
 n_{(j)}& := \#Id(j) \\
\bar{y}_{(j)} &:=\overline{\{y_{i} ; z_{i}=j\}} \\
S_{(j)}^{2} &:=\sum_{i \in Id(j)}\left(y_{i}-\mu_{j}\right)^{2}
\end{aligned}$$

* (3)

$$\mu_{j} \sim \N\left(\frac{n_{(j)} \bar{y}_{(j)} / \sigma_{j}^{2}+\mu_{0} / \sigma_{0}^{2}}{n_{(j)} / \sigma_{j}^{2}+1 / \sigma_{0}^{2}}\right.\left., \frac{1}{n_{(j)} / \sigma_{j}^{2}+1 / \sigma_{0}^{2}}\right) $$

* (4)

$$\sigma_{j}^{-2} \sim \textbf{Gamma}\left(a_{0}+\frac{n_{(j)}}{2}, b_{0}+\frac{1}{2} S_{(j)}^{2}\right)$$

* (5)

$$\pi\sim\textbf{Dirichlet}(\alpha_1+n_{(1)},\cdots,\alpha_K+n_{(K)})$$

### 3. Variational inference

Given below variational distribution:

$$\prod_{k=1}^K q(\mu_k| u_k, s^2_k) \times \prod_{k=1}^K q(\sigma^{-2}_k| a_k, b_k)
 q((\pi_1,\cdots, \pi_K)| \beta_1,\cdots, \beta_K) \times
\prod_{n=1}^N q(z_n | p_{n1}, \cdots, p_{nK}) $$

$q(\mu_k\| u_k, s^2_k)$ is a Gaussian distribution with parameter $u_k$ and $s^2_k$

$q(\sigma^{-2}_k\| a_k, b_k)$ is a Gamma distribution with parameter $a_k$ and $b_k$

$q((\pi_1,\cdots, \pi_K)\| \beta_1,\cdots, \beta_K) $ is a Dirichlet distribution with parameters $\beta_1,\cdots, \beta_K$

$q(z_n \| p_{n1}, \cdots, p_{nK})$ is a discrete distribution with parameter $p_{n1},\cdots, p_{nK}$, such that $P(z_n=k) = p_{nk}$

I will do this using ELBO and Monte carlo.
