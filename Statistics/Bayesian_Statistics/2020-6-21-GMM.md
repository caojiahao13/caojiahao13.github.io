---
layout: default
title: "Jiahao Cao's homepage"
description: Bayesian Statistics
theme: jekyll-theme-cayman
---
$
\newcommand{\E}{\mathbb{E}}
\newcommand{\P}{\mathbb{P}}
\newcommand{\N}{\mathcal{N}}
$
In my Bayesian class I was asked to do inference based Gaussian Mixture Model(GMM):

$$\begin{align*}
&Y_n | Z_n =k, \mu_k, \sigma_k^2 \sim \N(\mu_k, \sigma_k^2),\quad n\in\{1,2,\cdots,N\} \\
&Z_n\in\{1,\cdots, K\} \text{ and } P(Z_n=k) =\pi_k.
\end{align*}$$

Where $Y_1,\cdots,Y_N$ are observations and $Z,\mu,\sigma^2,\pi$ are hidden. Our goal is make inference about $\mu,\sigma,\pi$ and $Z$.

We will use three different methods, namely

* EM algorithm
* Gibbs sampling with conjugate priors
* Variational inference with mean-field variational distribution

## 1. EM algorithm

The log-likelihood is:

$$
\log p(y, z | \mu, \sigma, \pi) \equiv \sum_{i=1}^{n} \sum_{j=1}^{K} 1_{\left[z_{i}=j\right]}\left\{\log \pi_{j}-\frac{1}{2} \log \sigma_{j}^{2}-\frac{1}{2} \frac{\left(y_{i}-\mu_{j}\right)^{2}}{\sigma_j^{2}}\right\}
$$

Write $\theta = (\mu\,\sigma^2,\pi)$.

### 1.1 E step

$$Q(\theta,\theta'):=\mathbb{E}\bigg\{\log p(y, z | \theta)\bigg| Y=y,\theta'\bigg\}=\sum_{i=1}^{n} \sum_{j=1}^{K} \mathbb{E}\bigg\{1_{\left[z_{i}=j\right]}\bigg| Y=y,\theta'\bigg\}\left\{\log \pi_{j}-\frac{1}{2} \log \sigma_{j}^{2}-\frac{1}{2} \frac{\left(y_{i}-\mu_{j}\right)^{2}}{\sigma_j^{2}}\right\}$$

So we need to find the conditional probability of $\{z_i=j\}$ given $\theta'$ and observations $y$. Notice the joint distribution of $(y, z | \mu, \sigma, \pi)$ shows that $(y_i,z_i) $
 are independent. So

 $$w_{ij}:=\P(z_i=j|y,\theta') =\P(z_i=j|y_i,\theta') =\frac{\pi'_{j} \N\left(y_{i} ; \mu'_{j}, \sigma_{j}^{'2}\right)}{\sum_{q=1}^{K} \pi'_{q} \N\left(y_{i}, \mu'_{q}, \sigma_{q}^{'2}\right)}$$

### 1.2 M step

$$\begin{array}{l}
(1):\quad\frac{\partial Q\left(\theta, \theta^{\prime}\right)}{\partial \pi_{j}}=\frac{1}{\pi_{j}}\left(\sum_{i=1}^{n} w_{i j}\right)=0 \\
(2):\quad\frac{\partial Q\left(\theta, \theta^{\prime}\right)}{\partial \sigma_{j}^{2}}=\sum_{i=1}^{n} w_{i j}\left[-\frac{1}{2 \sigma_{j}^{2}}+\frac{\left(y_{i}-u_{j}\right)^{2}}{2\left(\sigma_j^{2}\right)^{2}}\right]=0 \\
(3):\quad\frac{\partial Q\left(\theta, \theta^{\prime}\right)}{\partial u_{j}}=\sum_{i=1}^{n} w_{i j} \cdot \frac{y_{i}-u_{j}}{\sigma_{j}^{2}}=0 \Rightarrow \mu_{j}=\frac{\sum_{i=1}^{n} w_{i j} y_{i}}{\sum_{i=1}^{n} w_{i j}}
\end{array}$$

$$\begin{array}{l}
(2)\Rightarrow \quad \sum_{i=1}^{n} w_{i j}=\frac{1}{\sigma_{j}^{2}}\left[\sum_{i=1}^N w_{ij}(y_i-\mu_j)^2\right] \\
\quad\quad\Rightarrow \sigma_{j}^{2}=\left(\sum_{i=1}^{n} w_{i j}\right)^{-1}\left[\sum_{i=1}^N w_{ij}(y_i-\mu_j)^2\right] \\
(1)\Rightarrow \quad \frac{\partial Q(\theta, \theta^{\prime})}{\partial \pi_{j}}=\frac{1}{\pi_{j}}\left(\sum_{i=1}^{n} w_{i j}\right)-\frac{\sum_{i=1}^{n} w_{i k}}{1-\pi_{1}-\cdots-\pi_{k-1}}=0 \Rightarrow \pi_{j}=\frac{\pi_K\left(\sum_{i=1}^{n} w_{i j}\right)}{\sum_{i=1}^{n} w_{i K}}\\
\quad\quad\Rightarrow \pi_j = \frac{1}{N}\sum_{i=1}^{N} w_{i j}
\end{array}$$

In summary:

$$w_{ij}=\frac{\pi'_{j} \N\left(y_{i} ; \mu'_{j}, \sigma_{j}^{'2}\right)}{\sum_{q=1}^{K} \pi'_{q} \N\left(y_{i}, \mu'_{q}, \sigma_{q}^{'2}\right)}$$

$$
\begin{cases}
\mu_{j}=\frac{\sum_{i=1}^{n} w_{i j} y_{i}}{\sum_{i=1}^{n} w_{i j}}\\
\sigma_{j}^{2}=\left(\sum_{i=1}^{n} w_{i j}\right)^{-1}\left[\sum_{i=1}^N w_{ij}(y_i-\mu_j)^2\right]\\
\pi_j = \frac{1}{N}\sum_{i=1}^{N} w_{i j}
\end{cases}$$

<!--see http://www.cse.psu.edu/~rtc12/CSE586/lectures/EMLectureFeb3.pdf for the answer.-->



## 2. Gibbs sampling

Now given the model and the corresponding priors:

$$\begin{align*}
&Y_n | Z_n =k, \mu_k, \sigma_k^2 \sim \N(\mu_k, \sigma_k^2) \\
&Z_n\in\{1,\cdots, K\} \text{ and } P(Z_n=k) =\pi_k \\
& \mu_k \sim \N(\mu_0, \sigma^2_0) \\
& \sigma_k^{-2} \sim \text{Gamma}(a_0, b_0) \\
& (\pi_1, \cdots, \pi_K)\sim \text{Dirichlet}(\alpha_1,\cdots,\alpha_K).
\end{align*}$$

We need to derive all the full conditional distributions. This is quite easy because there are some conjugacy.

Write

$$\begin{aligned}
Id(j):&=\left\{i: z_{i}=j\right\} \\
 n_{(j)}& := \#Id(j) \\
\bar{y}_{(j)} &:=\overline{\{y_{i} ; z_{i}=j\}} \\
S_{(j)}^{2} &:=\sum_{i \in Id(j)}\left(y_{i}-\mu_{j}\right)^{2}
\end{aligned}$$

* (1)

$$\mu_{j} \sim \N\left(\frac{n_{(j)} \bar{y}_{(j)} / \sigma_{j}^{2}+\mu_{0} / \sigma_{0}^{2}}{n_{(j)} / \sigma_{j}^{2}+1 / \sigma_{0}^{2}}\right.\left., \frac{1}{n_{(j)} / \sigma_{j}^{2}+1 / \sigma_{0}^{2}}\right) $$

* (2)

$$\sigma_{j}^{-2} \sim \textbf{Gamma}\left(a_{0}+\frac{n_{(j)}}{2}, b_{0}+\frac{1}{2} S_{(j)}^{2}\right)$$

* (3)

$$\pi\sim\textbf{Dirichlet}(\alpha_1+n_{(1)},\cdots,\alpha_K+n_{(K)})$$


* (4)

$$
p(z |-)=\prod_{i=1}^N\frac{  \prod_{j=1}^{K}\left[\pi_{j} \N\left(y_{i} ; \mu_{j} , \sigma^{2}_j\right)\right]^{I_{\left(z_{i}=j\right) }}}{\sum_{j=1}^{K} \pi_{j} \N\left(y_{i} ; \mu_{j}, \sigma_{j}^{2}\right)}
$$

### 3. Variational inference

The model is

$$\begin{align*}
&Y_n | Z_n =k, \mu_k, \sigma_k^2 \sim \N(\mu_k, \sigma_k^2) \\
&Z_n\in\{1,\cdots, K\} \text{ and } P(Z_n=k) =\pi_k \\
& \mu_k \sim \N(\mu_0, \sigma^2_0) \\
& \sigma_k^{-2} \sim \text{Gamma}(a_0, b_0) \\
& (\pi_1, \cdots, \pi_K)\sim \text{Dirichlet}(\alpha_1,\cdots,\alpha_K).
\end{align*}$$

Given below variational distribution:

$$\prod_{k=1}^K q(\mu_k| u_k, s^2_k) \times \prod_{k=1}^K q(\sigma^{-2}_k| a_k, b_k)
 q((\pi_1,\cdots, \pi_K)| \beta_1,\cdots, \beta_K) \times
\prod_{n=1}^N q(z_n | p_{n1}, \cdots, p_{nK}) $$

$q(\mu_k\| u_k, s^2_k)$ is a Gaussian distribution with parameter $u_k$ and $s^2_k$

$q(\sigma^{-2}_k\| a_k, b_k)$ is a Gamma distribution with parameter $a_k$ and $b_k$

$q((\pi_1,\cdots, \pi_K)\| \beta_1,\cdots, \beta_K) $ is a Dirichlet distribution with parameters $\beta_1,\cdots, \beta_K$

$q(z_n \| p_{n1}, \cdots, p_{nK})$ is a discrete distribution with parameter $p_{n1},\cdots, p_{nK}$, such that $P(z_n=k) = p_{nk}$

Write $h=(\mu,\sigma,\pi,z)$ represents all hidden random variables and $\lambda = (u,s,a,b,\beta,p)$ represents all variational parameters.

The variational inference is to find parameters $\lambda$ to maximize the ELBO:

$$\mathscr{L}(\lambda):=E_{q(h | \lambda)}[\log p(y, h)-\log q(h | \lambda)]$$

To use gradient based optimizer, we need to find the derivatives of the ELBO. It can be shown that:

$$\nabla_\lambda \mathscr{L}(\lambda)= \mathbb{E}_{q(h | \lambda)}\left[\nabla_{\lambda} \log q(h | \lambda)(\log p(y, h)-\log q(h | \lambda))\right]$$


First we write down $\log f(y, h)$ and $\log q(h \| \lambda)$:

$$
\begin{aligned}
\log f(y, h) \equiv &\left[\sum_{i=1}^{N}\left(-\frac{1}{2} \log \sigma_{z_{i}}^{2}-\frac{\left(y_{i}-\mu_{z_{i}}\right)^{2}}{2 \sigma_{z_{i}}^{2}}\right)+\sum_{j=1}^{K}\left(-\frac{\left(\mu_j-\mu_{0}\right)^{2}}{2 \sigma_{0}^{2}}\right)+\sum_{j=1}^{K}\left(\left(a_{0}-1\right) \log \sigma_{j}^{-2}-b_{0} \sigma_{j}^{-2}\right)\right]\\
&+\left[\sum_{i=1}^{N} \sum_{j=1}^{K} 1_{\left[z_{i}= j\right]} \log\pi_{j}\right]+\left[\sum_{j=1}^{K}\left(\alpha_{j}-1\right) \log \pi_{j}\right]
\end{aligned}
$$

$$\begin{aligned}
\log q(h | \lambda)  &\equiv\left\{\sum_{j=1}^{K}\left(-\frac{1}{2} \log s_{j}^{2}-\frac{\left(\mu_{j}-u_{j}\right)^{2}}{2 s_{j}^ 2}\right)+\sum_{j=1}^{K}\left(a_j\log(b_j)-\log(\Gamma(a_j)) + \left(a_{j}-1\right) \log \sigma_{j}^{-2}-b_{j} \sigma_{j}^{-2}\right)\right.\\
&\left.+\left[\sum_{i=1}^{N} \sum_{j=1}^{K} 1_{\left[z_{i}=j\right]} \log p_{i j}\right]+\left[\log\left(\Gamma(\sum_{l=1}^K \beta_l)\right) - \sum_{l=1}^K\log\Gamma(\beta_l) + \sum_{j=1}^{K}\left(\beta_{j}-1\right) \log \pi_{j}\right]\right\}
\end{aligned}
$$




<!--
$$\begin{aligned}
\log f(y, h)-\log q(h | \lambda)  \equiv &\left\{\left[\sum_{i=1}^{N}\left(-\frac{1}{2} \log \sigma_{z_{i}}^{2}-\frac{\left(y_{i}-\mu_{z_{i}}\right)^{2}}{2 \sigma_{z_{i}}^{2}}\right)+\sum_{j=1}^{K}\left(-\frac{\left(\mu_j-\mu_{0}\right)^{2}}{2 \sigma_{0}^{2}}\right)+\sum_{j=1}^{K}\left(\left(a_{0}-1\right) \log \sigma_{j}^{-2}-b_{0} \sigma_{j}^{-2}\right)\right]\right.\\
&\left.+\left[\sum_{i=1}^{N} \sum_{j=1}^{K} 1_{\left[z_{i}= j\right]} \pi_{j}\right]+\left[\sum_{j=1}^{K}\left(\alpha_{j}-1\right) \log \pi_{j}\right]\right\} \\
&-\left\{\sum_{j=1}^{K}\left(-\frac{1}{2} \log s_{j}^{2}-\frac{\left(\mu_{j}-u_{j}\right)^{2}}{2 s_{j}^ 2}\right)+\sum_{j=1}^{K}\left(\left(a_{j}-1\right) \log \sigma_{j}^{-2}-b_{j} \sigma_{j}^{-2}\right)\right.\\
&\left.+\left[\sum_{i=1}^{N} \sum_{j=1}^{K} 1_{\left[z_{i}=j\right]} p_{i j}\right]+\left[\sum_{j=1}^{K}\left(\beta_{j}-1\right) \log \pi_{j}\right]\right\}
\end{aligned}$$
-->


$$
\begin{cases}
\frac{\partial}{\partial u_{j}} \log q(h | \lambda)=\frac{\mu_{j}-u_{j}}{s_{j}^{2}} \\
\frac{\partial}{\partial s_{j}^{-2}} \log q(h | \lambda)=\frac{1}{2 s_{j}^{-2}}-\frac{1}{2}\left(\mu_{j}-u_{j}\right)^{2} \\
\frac{\partial}{\partial a_{j}} \log \operatorname{l}(h | \lambda)=\log \sigma_{j}^{-2} +\log b_j - \frac{\Gamma'(a_j)}{\Gamma(a_j)}\\
\frac{\partial}{\partial b_{j}} \log q(h | \lambda)=-\sigma_{j}^{-2}+\frac{a_j}{b_j} \\
\frac{\partial}{\partial \beta_{j}} \log q(h | \lambda)=\log \pi_{j} +\frac{\Gamma'(\sum_{l=1}^K \beta_l)}{\Gamma(\sum_{l=1}^K \beta_l)}-  \frac{\Gamma'(\beta_j)}{\Gamma(\beta_j)}\\
\frac{\partial}{\partial p_{i j}} \log q(h | \lambda)=\frac{1}{p_{ij}}1_{[z_i=j]}-\frac{1}{p_{iK}}1_{[z_i=K]}, \quad j \in\{1,2, \cdots, K-1\}
\end{cases}
$$

We can compute the derivatives of ELBO by deriving their close forms or using Monte Carlo.  In this case, using direct expectation to compute $\nabla_\lambda \mathscr{L}(\lambda)$ is really complex so we just do Monte Carlo.


$$\begin{aligned} \nabla_{\mu_j} \mathscr{L}(\lambda) &= \mathbb{E}_{q(h|\lambda)}\left\{\frac{\mu_{j}-u_{j}}{s_{j}^{2}}\left[\sum_{i \in \mathrm{Id}(j)}\left(-\frac{\left(y_{i}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right)-\frac{\left(\mu_{j}-\mu_{0}\right)^{2}}{2 \sigma_{0}^{2}}+\frac{\left(\mu_{j}-u_{j}\right)^{2}}{2 s_j^2}\right]\right\} \\
&= \frac{1}{\sigma_{0}^{2}}\left(u_{0}-u_{j}\right)+\sum_{i \in I d(j)} \frac{a_{j}}{b_{j}}\left(y_{i}-u_{j}\right)
\end{aligned}$$

$$\begin{aligned}
\nabla_{s^{-2}_j} \mathscr{L}(\lambda)&=\mathbb{E}_{q(h|\lambda)}\left\{\left[\frac{1}{2 s_{j}^{-2}}-\frac{1}{2}\left(\mu_{j}-u_{j}\right)^{2}\right]\left[\sum_{i \in \mathrm{Id}(j)}\left(-\frac{\left(y_{i}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right)-\frac{\left(\mu_{j}-\mu_{0}\right)^{2}}{2 \sigma_{0}^{2}}+\frac{\left(\mu_{j}-u_{j}\right)^{2}}{2 s_j^2}\right]\right\}
\\
& = E_{q(h \mid \lambda)}\left\{\left[\frac{1}{2 s_{j}^{-2}}-\frac{1}{2}\left(\mu_{j}-u_{j}\right)^{2}\right]\left[\sum_{i \in I d(j)}\left(-\frac{\left(\mu_{j}-u_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right)-\frac{\left(\mu_{j}-u_{0}\right)^{2}}{2 \sigma_{0}^{2}}+\frac{\left(\mu_{j}-u_{j}\right)^{2}}{2 s_{j}^{2}}\right]\right.
\\
&=\mathbb{E}_{q(n \mid \lambda)}\left\{\frac{n_{(j)}}{2 \sigma_{j}^{2}}\left[-\frac{\left(\mu_{j}-u_{j}\right)^{2}}{2 s_{j}-2}+\frac{1}{2}\left(\mu_{j}-u_{j}\right)^{4}\right]+\frac{1}{4}\left(\frac{1}{s_{j}^{2}}-\frac{1}{\sigma_{0}^{2}}\right)\left[s_{j}^{4}-3 s_{j}^{2}\right]\right. \\
&=\frac{n_{(j)} a_{j}}{b_{j}}\left[-\frac{s_{j}^{2}}{2 s_{j}^{-2}}+\frac{1}{2} \cdot 3 s_{j}^{2}\right]-\frac{1}{2}\left(s_{j}^{-2}-\sigma_{0}^{-2}\right) s_{j}^{4} \\
&=\frac{n_{j} a_{j}}{b_{j}} s_{j}^{4}+\frac{1}{2}\left(\sigma_{0}^{-2} s_{j}^{4}-s_{j}^{2}\right)

\end{aligned}$$

$$\nabla_{a_j} \mathscr{L}(\lambda)=\mathbb{E}_{q(h|\lambda)}\left[\log \sigma_{j}^{-2} +\log b_j - \frac{\Gamma'(a_j)}{\Gamma(a_j)}\right]\left[\sum_{i\in Id(j)}\left(-\frac{1}{2} \log \sigma_{j}^{2}-\frac{\left(y_{i}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right)+\sum_{j=1}^{K}\left(\left(a_{0}-1\right) \log \sigma_{j}^{-2}-b_{0} \sigma_{j}^{-2}\right)-\sum_{j=1}^{K}\left( \left(a_{j}-1\right) \log \sigma_{j}^{-2}-b_{j} \sigma_{j}^{-2}\right)\right]$$

$$\nabla_{b_j} \mathscr{L}(\lambda)= \mathbb{E}_{q(h|\lambda)}\left[ -\sigma_{j}^{-2}+\frac{a_j}{b_j}   \right]\left[\sum_{i\in Id(j)}\left(-\frac{1}{2} \log \sigma_{j}^{2}-\frac{\left(y_{i}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right)+\sum_{j=1}^{K}\left(\left(a_{0}-1\right) \log \sigma_{j}^{-2}-b_{0} \sigma_{j}^{-2}\right)-\sum_{j=1}^{K}\left( \left(a_{j}-1\right) \log \sigma_{j}^{-2}-b_{j} \sigma_{j}^{-2}\right)   \right] $$

$$\nabla_{\beta_j} \mathscr{L}(\lambda)= \mathbb{E}_{q(h|\lambda)}\left[\log \pi_{j} +\frac{\Gamma'(\sum_{l=1}^K \beta_l)}{\Gamma(\sum_{l=1}^K \beta_l)}-  \frac{\Gamma'(\beta_j)}{\Gamma(\beta_j)}\right]\left[\sum_{i=1}^{N} \sum_{j=1}^{K} 1_{\left[z_{i}= j\right]} \log\pi_{j}+\sum_{j=1}^{K}\left(\alpha_{j}-1\right) \log \pi_{j}-\sum_{j=1}^{K}\left(\beta_{j}-1\right) \log \pi_{j}\right]$$

$$\nabla_{p_{ij}} \mathscr{L}(\lambda)= \mathbb{E}_{q(h|\lambda)}\left[\frac{1}{p_{ij}}1_{[z_i=j]}-\frac{1}{p_{iK}}1_{[z_i=K]}\right]\left[\sum_{i=1}^{N}\left(-\frac{1}{2} \log \sigma_{z_{i}}^{2}-\frac{\left(y_{i}-\mu_{z_{i}}\right)^{2}}{2 \sigma_{z_{i}}^{2}}\right) +\sum_{i=1}^{N} \sum_{j=1}^{K} 1_{\left[z_{i}= j\right]} \log\pi_{j}- \sum_{i=1}^{N} \sum_{j=1}^{K} 1_{\left[z_{i}=j\right]} \log p_{i j}\right]
, \quad j \in\{1,2, \cdots, K-1\}$$
